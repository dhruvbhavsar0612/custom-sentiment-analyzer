{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhQ+srGBxS8lSheMQnXG2C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5642ba05d71b4e97a1b78a15e815a8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9537568d44f04370b5f19aafaf9e58c5",
              "IPY_MODEL_a6eaf2c9f5dd4673952f12dc36afadfc",
              "IPY_MODEL_ca8f85231ab546d0a1853728b5688fef"
            ],
            "layout": "IPY_MODEL_0d453746e8324f29bcb9b6cf0970ab15"
          }
        },
        "9537568d44f04370b5f19aafaf9e58c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fbc959da6e948a9ac01f4ea069f03b0",
            "placeholder": "​",
            "style": "IPY_MODEL_48f36a40ec6f45aab9dfba5f97cf97f6",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a6eaf2c9f5dd4673952f12dc36afadfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c976b9f0c56b49eebcf3e8c3454cab27",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e6ac9d996594b4db51be94839e7d90d",
            "value": 2
          }
        },
        "ca8f85231ab546d0a1853728b5688fef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01267cb8a49343f3af05d98edf4b19fc",
            "placeholder": "​",
            "style": "IPY_MODEL_7f4ce1e415524bf8b893c27db6d888f2",
            "value": " 2/2 [01:00&lt;00:00, 27.51s/it]"
          }
        },
        "0d453746e8324f29bcb9b6cf0970ab15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fbc959da6e948a9ac01f4ea069f03b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f36a40ec6f45aab9dfba5f97cf97f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c976b9f0c56b49eebcf3e8c3454cab27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6ac9d996594b4db51be94839e7d90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01267cb8a49343f3af05d98edf4b19fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f4ce1e415524bf8b893c27db6d888f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02226a60ca674e22baf8a2bb4121592a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40f477daf85a45dd9ef4ec0d5853446d",
              "IPY_MODEL_831ed1b4a991467786bce2478619ec44",
              "IPY_MODEL_e85a463e277141f48ab8f218c5a3b6be"
            ],
            "layout": "IPY_MODEL_672bea8b8f9f4643a48f2e734fd4bc0b"
          }
        },
        "40f477daf85a45dd9ef4ec0d5853446d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fdddcfbcc924493bfde90c7d0f2801e",
            "placeholder": "​",
            "style": "IPY_MODEL_3c84a1cf817f445e89bdec3944c3ac6c",
            "value": "Downloading data: 100%"
          }
        },
        "831ed1b4a991467786bce2478619ec44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_654a81d6db8849008c9d1c11c217df21",
            "max": 33065902,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04396cea854048dcaf29791878ff184a",
            "value": 33065902
          }
        },
        "e85a463e277141f48ab8f218c5a3b6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81962322ff3c41b4956594dcff3c05f5",
            "placeholder": "​",
            "style": "IPY_MODEL_7998a4e8e4b74917acd69590a2ab99ea",
            "value": " 33.1M/33.1M [00:03&lt;00:00, 8.92MB/s]"
          }
        },
        "672bea8b8f9f4643a48f2e734fd4bc0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fdddcfbcc924493bfde90c7d0f2801e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c84a1cf817f445e89bdec3944c3ac6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "654a81d6db8849008c9d1c11c217df21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04396cea854048dcaf29791878ff184a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81962322ff3c41b4956594dcff3c05f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7998a4e8e4b74917acd69590a2ab99ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c8c6ecdedc1438589a7b0f786a2b65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_373d78eae18b42f78f7e5caab0ce099f",
              "IPY_MODEL_a4c0f7264efa481392df96f3a1d06488",
              "IPY_MODEL_683655a77f5647f4a31941e50fb039e9"
            ],
            "layout": "IPY_MODEL_1b6f0ad827f44b7896a906127025beaa"
          }
        },
        "373d78eae18b42f78f7e5caab0ce099f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4fc7c5f36f4970a497e37ac63dae8a",
            "placeholder": "​",
            "style": "IPY_MODEL_f7ac5e5134a9457d8dac151439dc3239",
            "value": "Generating train split: "
          }
        },
        "a4c0f7264efa481392df96f3a1d06488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14adfb7127a1472ca4344e0d8356195f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36c06bc3f5354ed8825e7f251626b32b",
            "value": 1
          }
        },
        "683655a77f5647f4a31941e50fb039e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24fb01783e854c74a74825c761de5bd7",
            "placeholder": "​",
            "style": "IPY_MODEL_1e02cf5f9da24f51bbf9062a53899a67",
            "value": " 23110/0 [00:00&lt;00:00, 59662.42 examples/s]"
          }
        },
        "1b6f0ad827f44b7896a906127025beaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4fc7c5f36f4970a497e37ac63dae8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ac5e5134a9457d8dac151439dc3239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14adfb7127a1472ca4344e0d8356195f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "36c06bc3f5354ed8825e7f251626b32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24fb01783e854c74a74825c761de5bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e02cf5f9da24f51bbf9062a53899a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvbhavsar0612/custom-sentiment-analyzer/blob/master/research/llama_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3ywh80EACQs",
        "outputId": "65e35e7e-4f0e-42f5-fef2-f956de251593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.0.1)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word embeddings\n",
        "\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUfrqtgRAN3O",
        "outputId": "9ee31d2e-5c84-4648-bf59-86c25597196e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIhU32QvAek_",
        "outputId": "ee28a4cb-5ebd-47f2-88d5-7885004a8a7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_index in /usr/local/lib/python3.10/dist-packages (0.9.40)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.9.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.6.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.26.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama_index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (3.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama_index) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.3.post1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama_index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "Lk_Fv8oRAquG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader(\"/content/Data\").load_data()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXzIuEPhA-xf",
        "outputId": "8fa89a72-cf47-4fa6-f46c-a0f0069e5ea7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='10b49be6-07af-4117-860c-dc60c62de085', embedding=None, metadata={'page_label': '1', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data\\nLihe Yang1Bingyi Kang2†Zilong Huang2Xiaogang Xu3,4Jiashi Feng2Hengshuang Zhao1†\\n1The University of Hong Kong2TikTok3Zhejiang Lab4Zhejiang University\\n†corresponding authors\\nhttps://depth-anything.github.io\\nFigure 1. Our model exhibits impressive generalization ability across extensive unseen scenes. Left two columns: COCO [ 36].Middle two:\\nSA-1B [ 27] (a hold-out unseen set). Right two: photos captured by ourselves. Our model works robustly in low-light environments (1st and\\n3rd column), complex scenes (2nd and 5th column), foggy weather (5th column), and ultra-remote distance (5th and 6th column), etc.\\nAbstract\\nThis work presents Depth Anything1, a highly practical\\nsolution for robust monocular depth estimation. Without pur-\\nsuing novel technical modules, we aim to build a simple yet\\npowerful foundation model dealing with any images under\\nany circumstances. To this end, we scale up the dataset by\\ndesigning a data engine to collect and automatically anno-\\ntate large-scale unlabeled data ( ∼62M), which significantly\\nenlarges the data coverage and thus is able to reduce the\\ngeneralization error. We investigate two simple yet effective\\nstrategies that make data scaling-up promising. First, a more\\nchallenging optimization target is created by leveraging data\\naugmentation tools. It compels the model to actively seek\\nextra visual knowledge and acquire robust representations.\\nSecond, an auxiliary supervision is developed to enforce\\nthe model to inherit rich semantic priors from pre-trained\\nencoders. We evaluate its zero-shot capabilities extensively,\\nincluding six public datasets and randomly captured photos.\\nIt demonstrates impressive generalization ability (Figure 1).\\nFurther, through fine-tuning it with metric depth information\\nfrom NYUv2 and KITTI, new SOTAs are set. Our better depth\\nmodel also results in a better depth-conditioned ControlNet.\\nOur models are released here.\\nThe work was done during an internship at TikTok.\\n1While the grammatical soundness of this name may be questionable,\\nwe treat it as a whole and pay homage to Segment Anything [27].1. Introduction\\nThe field of computer vision and natural language processing\\nis currently experiencing a revolution with the emergence of\\n“foundation models” [ 6] that demonstrate strong zero-/few-\\nshot performance in various downstream scenarios [ 45,59].\\nThese successes primarily rely on large-scale training data\\nthat can effectively cover the data distribution. Monocular\\nDepth Estimation (MDE), which is a fundamental problem\\nwith broad applications in robotics [ 66], autonomous driv-\\ning [ 64,80], virtual reality [ 48],etc., also requires a foun-\\ndation model to estimate depth information from a single\\nimage. However, this has been underexplored due to the\\ndifficulty of building datasets with tens of millions of depth\\nlabels. MiDaS [ 46] made a pioneering study along this di-\\nrection by training an MDE model on a collection of mixed\\nlabeled datasets. Despite demonstrating a certain level of\\nzero-shot ability, MiDaS is limited by its data coverage, thus\\nsuffering disastrous performance in some scenarios.\\nIn this work, our goal is to build a foundation model for\\nMDE capable of producing high-quality depth information\\nfor any images under any circumstances. We approach this\\ntarget from the perspective of dataset scaling-up. Tradition-\\nally, depth datasets are created mainly by acquiring depth\\ndata from sensors [ 18,55], stereo matching [ 15], or SfM [ 33],\\nwhich is costly, time-consuming, or even intractable in partic-\\nular situations. We instead, for the first time, pay attention to\\nlarge-scale unlabeled data. Compared with stereo images or\\n1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14d62e2b-7894-4e5b-85fd-0b48ff13aeff', embedding=None, metadata={'page_label': '2', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='labeled images from depth sensors, our used monocular unla-\\nbeled images exhibit three advantages: (i) ( simple and cheap\\nto acquire ) Monocular images exist almost everywhere, thus\\nthey are easy to collect, without requiring specialized de-\\nvices. (ii) ( diverse ) Monocular images can cover a broader\\nrange of scenes, which are critical to the model generaliza-\\ntion ability and scalability. (iii) ( easy to annotate ) We can\\nsimply use a pre-trained MDE model to assign depth labels\\nfor unlabeled images, which only takes a feedforward step.\\nMore than efficient, this also produces denser depth maps\\nthan LiDAR [ 18] and omits the computationally intensive\\nstereo matching process.\\nWe design a data engine to automatically generate depth\\nannotations for unlabeled images, enabling data scaling-up\\nto arbitrary scale. It collects 62M diverse and informative im-\\nages from eight public large-scale datasets, e.g., SA-1B [ 27],\\nOpen Images [ 30], and BDD100K [ 82]. We use their raw\\nunlabeled images without any forms of labels. Then, in or-\\nder to provide a reliable annotation tool for our unlabeled\\nimages, we collect 1.5M labeled images from six public\\ndatasets to train an initial MDE model. The unlabeled im-\\nages are then automatically annotated and jointly learned\\nwith labeled images in a self-training manner [31].\\nDespite all the aforementioned advantages of monocular\\nunlabeled images, it is indeed not trivial to make positive use\\nof such large-scale unlabeled images [ 73,90], especially in\\nthe case of sufficient labeled images and strong pre-training\\nmodels. In our preliminary attempts, directly combining la-\\nbeled and pseudo labeled images failed to improve the base-\\nline of solely using labeled images. We conjecture that, the\\nadditional knowledge acquired in such a naive self-teaching\\nmanner is rather limited. To address the dilemma, we pro-\\npose to challenge the student model with a more difficult\\noptimization target when learning the pseudo labels. The\\nstudent model is enforced to seek extra visual knowledge\\nand learn robust representations under various strong pertur-\\nbations to better handle unseen images.\\nFurthermore, there have been some works [ 9,21] demon-\\nstrating the benefit of an auxiliary semantic segmentation\\ntask for MDE. We also follow this research line, aiming to\\nequip our model with better high-level scene understanding\\ncapability. However, we observed when an MDE model is\\nalready powerful enough, it is hard for such an auxiliary\\ntask to bring further gains. We speculate that it is due to\\nsevere loss in semantic information when decoding an im-\\nage into a discrete class space. Therefore, considering the\\nexcellent performance of DINOv2 in semantic-related tasks,\\nwe propose to maintain the rich semantic priors from it with\\na simple feature alignment loss. This not only enhances the\\nMDE performance, but also yields a multi-task encoder for\\nboth middle-level and high-level perception tasks.\\nOur contributions are summarized as follows:\\n•We highlight the value of data scaling-up of massive,cheap, and diverse unlabeled images for MDE.\\n•We point out a key practice in jointly training large-\\nscale labeled and unlabeled images. Instead of learning\\nraw unlabeled images directly, we challenge the model\\nwith a harder optimization target for extra knowledge.\\n•We propose to inherit rich semantic priors from pre-\\ntrained encoders for better scene understanding, rather\\nthan using an auxiliary semantic segmentation task.\\n•Our model exhibits stronger zero-shot capability than\\nMiDaS-BEiT L-512 [5]. Further, fine-tuned with metric\\ndepth, it outperforms ZoeDepth [4] significantly.\\n2. Related Work\\nMonocular depth estimation (MDE). Early works [ 23,37,\\n51] primarily relied on handcrafted features and traditional\\ncomputer vision techniques. They were limited by their re-\\nliance on explicit depth cues and struggled to handle complex\\nscenes with occlusions and textureless regions.\\nDeep learning-based methods have revolutionized monoc-\\nular depth estimation by effectively learning depth represen-\\ntations from delicately annotated datasets [ 18,55]. Eigen\\net al. [17] first proposed a multi-scale fusion network to\\nregress the depth. Following this, many works consistently\\nimprove the depth estimation accuracy by carefully design-\\ning the regression task as a classification task [ 3,34], in-\\ntroducing more priors [ 32,54,76,83], and better objective\\nfunctions [ 68,78],etc. Despite the promising performance,\\nthey are hard to generalize to unseen domains.\\nZero-shot depth estimation. Our work belongs to this re-\\nsearch line. We aim to train an MDE model with a diverse\\ntraining set and thus can predict the depth for any given im-\\nage. Some pioneering works [ 10,67] explored this direction\\nby collecting more training images, but their supervision is\\nvery sparse and is only enforced on limited pairs of points.\\nTo enable effective multi-dataset joint training, a mile-\\nstone work MiDaS [ 46] utilizes an affine-invariant loss to\\nignore the potentially different depth scales and shifts across\\nvarying datasets. Thus, MiDaS provides relative depth infor-\\nmation. Recently, some works [ 4,22,79] take a step further\\nto estimate the metric depth. However, in our practice, we\\nobserve such methods exhibit poorer generalization ability\\nthan MiDaS, especially its latest version [ 5]. Besides, as\\ndemonstrated by ZoeDepth [ 4], a strong relative depth es-\\ntimation model can also work well in generalizable metric\\ndepth estimation by fine-tuning with metric depth informa-\\ntion. Therefore, we still follow MiDaS in relative depth\\nestimation, but further strengthen it by highlighting the value\\nof large-scale monocular unlabeled images.\\nLeveraging unlabeled data. This belongs to the research\\narea of semi-supervised learning [31, 56, 90], which is pop-\\nular with various applications [ 71,75]. However, existing\\n2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b817fe98-a71d-446f-adf1-2eeadd6b959c', embedding=None, metadata={'page_label': '3', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='works typically assume only limited images are available.\\nThey rarely consider the challenging but realistic scenario\\nwhere there are already sufficient labeled images but also\\nlarger-scale unlabeled images. We take this challenging di-\\nrection for zero-shot MDE. We demonstrate that unlabeled\\nimages can significantly enhance the data coverage and thus\\nimprove model generalization and robustness.\\n3. Depth Anything\\nOur work utilizes both labeled and unlabeled images to\\nfacilitate better monocular depth estimation (MDE). For-\\nmally, the labeled and unlabeled sets are denoted as Dl=\\n{(xi, di)}M\\ni=1andDu={ui}N\\ni=1respectively. We aim to\\nlearn a teacher model TfromDl. Then, we utilize Tto\\nassign pseudo depth labels for Du. Finally, we train a stu-\\ndent model Son the combination of labeled set and pseudo\\nlabeled set. A brief illustration is provided in Figure 2.\\n3.1. Learning Labeled Images\\nThis process is similar to the training of MiDaS [ 5,46].\\nHowever, since MiDaS did not release its code, we first\\nreproduced it. Concretely, the depth value is first transformed\\ninto the disparity space by d= 1/tand then normalized\\nto 0∼1 on each depth map. To enable multi-dataset joint\\ntraining, we adopt the affine-invariant loss to ignore the\\nunknown scale and shift of each sample:\\nLl=1\\nHWHWX\\ni=1ρ(d∗\\ni, di), (1)\\nwhere d∗\\nianddiare the prediction and ground truth, respec-\\ntively. And ρis the affine-invariant mean absolute error loss:\\nρ(d∗\\ni, di) =|ˆd∗\\ni−ˆdi|, where ˆd∗\\niandˆdiare the scaled and\\nshifted versions of the prediction d∗\\niand ground truth di:\\nˆdi=di−t(d)\\ns(d), (2)\\nwhere t(d)ands(d)are used to align the prediction and\\nground truth to have zero translation and unit scale:\\nt(d) =median (d), s(d) =1\\nHWHWX\\ni=1|di−t(d)|.(3)\\nTo obtain a robust monocular depth estimation model, we\\ncollect 1.5M labeled images from 6 public datasets. Details\\nof these datasets are listed in Table 1. We use fewer labeled\\ndatasets than MiDaS v3.1 [ 5] (12 training datasets), because\\n1) we do not use NYUv2 [ 55] and KITTI [ 18] datasets to\\nensure zero-shot evaluation on them, 2) some datasets are\\nnot available (anymore), e.g., Movies [ 46] and WSVD [ 61],\\nand 3) some datasets exhibit poor quality, e.g., RedWeb (also\\nlow resolution) [ 67]. Despite using fewer labeled images,Dataset Indoor Outdoor Label # Images\\nLabeled Datasets\\nBlendedMVS [77] ✓ ✓ Stereo 115K\\nDIML [13] ✓ ✓ Stereo 927K\\nHRWSI [68] ✓ ✓ Stereo 20K\\nIRS [62] ✓ Stereo 103K\\nMegaDepth [33] ✓ SfM 128K\\nTartanAir [63] ✓ ✓ Stereo 306K\\nUnlabeled Datasets\\nBDD100K [82] ✓ None 8.2M\\nGoogle Landmarks [65] ✓ None 4.1M\\nImageNet-21K [50] ✓ ✓ None 13.1M\\nLSUN [81] ✓ None 9.8M\\nObjects365 [53] ✓ ✓ None 1.7M\\nOpen Images V7 [30] ✓ ✓ None 7.8M\\nPlaces365 [88] ✓ ✓ None 6.5M\\nSA-1B [27] ✓ ✓ None 11.1M\\nTable 1. In total, our Depth Anything is trained on 1.5M labeled\\nimages and 62M unlabeled images jointly.\\nour easy-to-acquire and diverse unlabeled images will com-\\nprehend the data coverage and greatly enhance the model\\ngeneralization ability and robustness.\\nFurthermore, to strengthen the teacher model Tlearned\\nfrom these labeled images, we adopt the DINOv2 [ 43] pre-\\ntrained weights to initialize our encoder. In practice, we\\napply a pre-trained semantic segmentation model [ 70] to de-\\ntect the sky region, and set its disparity value as 0 (farthest).\\n3.2. Unleashing the Power of Unlabeled Images\\nThis is the main point of our work. Distinguished from prior\\nworks that laboriously construct diverse labeled datasets,\\nwe highlight the value of unlabeled images in enhancing\\nthe data coverage. Nowadays, we can practically build a\\ndiverse and large-scale unlabeled set from the Internet or\\npublic datasets of various tasks. Also, we can effortlessly\\nobtain the dense depth map of monocular unlabeled images\\nsimply by forwarding them to a pre-trained well-performed\\nMDE model. This is much more convenient and efficient\\nthan performing stereo matching or SfM reconstruction for\\nstereo images or videos. We select eight large-scale public\\ndatasets as our unlabeled sources for their diverse scenes.\\nThey contain more than 62M images in total. The details are\\nprovided in the bottom half of Table 1.\\nTechnically, given the previously obtained MDE teacher\\nmodel T, we make predictions on the unlabeled set Duto\\nobtain a pseudo labeled set ˆDu:\\nˆDu={(ui, T(ui))|ui∈ Du}N\\ni=1. (4)\\nWith the combination set Dl∪ˆDuof labeled images and\\npseudo labeled images, we train a student model Son it.\\n3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b119307d-689e-4b42-b929-cdedbc211a82', embedding=None, metadata={'page_label': '4', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='labeled image\\nunlabeled imageencoder\\n decoder\\nmanual label\\npseudo labelencoderteacher\\nmodelLiDAR, \\nmatching, \\nSfM, etc\\nsemantic\\npreservationlabeled prediction\\nunlabeled predictionsup\\nsupHRWSI: 102684_LookInStereoDotComDSCF0486\\nSA1B: sa_10000139\\nSfeature \\nalignmentFigure 2. Our pipeline. Solid line: flow of labeled images, dotted line: unlabeled images. We especially highlight the value of large-scale\\nunlabeled images. The Sdenotes adding strong perturbations (Section 3.2). To equip our depth estimation model with rich semantic priors,\\nwe enforce an auxiliary constraint between the online student model and a frozen encoder to preserve the semantic capability (Section 3.3).\\nFollowing prior works [ 74], instead of fine-tuning SfromT,\\nwe re-initialize Sfor better performance.\\nUnfortunately, in our pilot studies, we failed to gain im-\\nprovements with such a self-training pipeline, which indeed\\ncontradicts the observations when there are only a few la-\\nbeled images [ 56]. We conjecture that, with already suffi-\\ncient labeled images in our case, the extra knowledge ac-\\nquired from additional unlabeled images is rather limited.\\nEspecially considering the teacher and student share the\\nsame pre-training and architecture, they tend to make similar\\ncorrect or false predictions on the unlabeled set Du, even\\nwithout the explicit self-training procedure.\\nTo address the dilemma, we propose to challenge the stu-\\ndent with a more difficult optimization target for additional\\nvisual knowledge on unlabeled images. We inject strong per-\\nturbations to unlabeled images during training. It compels\\nour student model to actively seek extra visual knowledge\\nand acquire invariant representations from these unlabeled\\nimages. These advantages help our model deal with the open\\nworld more robustly. We introduce two forms of perturba-\\ntions: one is strong color distortions, including color jittering\\nand Gaussian blurring, and the other is strong spatial dis-\\ntortion, which is CutMix [ 84]. Despite the simplicity, the\\ntwo modifications make our large-scale unlabeled images\\nsignificantly improve the baseline of labeled images.\\nWe provide more details about CutMix. It was originally\\nproposed for image classification, and is rarely explored in\\nmonocular depth estimation. We first interpolate a random\\npair of unlabeled images uaandubspatially:\\nuab=ua⊙M+ub⊙(1−M), (5)\\nwhere Mis a binary mask with a rectangle region set as 1.\\nThe unlabeled loss Luis obtained by first computing\\naffine-invariant losses in valid regions defined by Mand\\n1−M, respectively:\\nLM\\nu=ρ\\x00\\nS(uab)⊙M, T (ua)⊙M\\x01\\n, (6)\\nL1−M\\nu =ρ\\x00\\nS(uab)⊙(1−M), T(ub)⊙(1−M)\\x01\\n,(7)where we omit thePand pixel subscript ifor simplicity.\\nThen we aggregate the two losses via weighted averaging:\\nLu=PM\\nHWLM\\nu+P(1−M)\\nHWL1−M\\nu. (8)\\nWe use CutMix with 50% probability. The unlabeled\\nimages for CutMix are already strongly distorted in color,\\nbut the unlabeled images fed into the teacher model Tfor\\npseudo labeling are clean, without any distortions.\\n3.3. Semantic-Assisted Perception\\nThere exist some works [ 9,21,28,72] improving depth es-\\ntimation with an auxiliary semantic segmentation task. We\\nbelieve that arming our depth estimation model with such\\nhigh-level semantic-related information is beneficial. Be-\\nsides, in our specific context of leveraging unlabeled images,\\nthese auxiliary supervision signals from other tasks can also\\ncombat the potential noise in our pseudo depth label.\\nTherefore, we made an initial attempt by carefully assign-\\ning semantic segmentation labels to our unlabeled images\\nwith a combination of RAM [ 86] + GroundingDINO [ 38] +\\nHQ-SAM [ 26] models. After post-processing, this yields a\\nclass space containing 4K classes. In the joint-training stage,\\nthe model is enforced to produce both depth and segmenta-\\ntion predictions with a shared encoder and two individual\\ndecoders. Unfortunately, after trial and error, we still could\\nnot boost the performance of the original MDE model. We\\nspeculated that, decoding an image into a discrete class space\\nindeed loses too much semantic information. The limited\\ninformation in these semantic masks is hard to further boost\\nour depth model, especially when our depth model has es-\\ntablished very competitive results.\\nTherefore, we aim to seek more informative semantic sig-\\nnals to serve as auxiliary supervision for our depth estimation\\ntask. We are greatly astonished by the strong performance\\nof DINOv2 models [ 43] in semantic-related tasks, e.g., im-\\nage retrieval and semantic segmentation, even with frozen\\nweights without any fine-tuning. Motivated by these clues,\\nwe propose to transfer its strong semantic capability to our\\n4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='79f01078-4cfe-464b-bf5b-8eefb21c9221', embedding=None, metadata={'page_label': '5', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Method EncoderKITTI [18] NYUv2 [55] Sintel [7] DDAD [20] ETH3D [52] DIODE [60]\\nAbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1\\nMiDaS v3.1 [5] ViT-L 0.127 0.850 0.048 0.980 0.587 0.699 0.251 0.766 0.139 0.867 0.075 0.942\\nDepth AnythingViT-S 0.080 0.936 0.053 0.972 0.464 0.739 0.247 0.768 0.127 0.885 0.076 0.939\\nViT-B 0.080 0.939 0.046 0.979 0.432 0.756 0.232 0.786 0.126 0.884 0.069 0.946\\nViT-L 0.076 0.947 0.043 0.981 0.458 0.760 0.230 0.789 0.127 0.882 0.066 0.952\\nTable 2. Zero-shot relative depth estimation. Better: AbsRel ↓,δ1↑. We compare with the best model from MiDaS v3.1. Note that MiDaS\\ndoes not strictly follow the zero-shot evaluation on KITTI and NYUv2, because it uses their training images. We provide three model scales\\nfor different purposes, based on ViT-S (24.8M), ViT-B (97.5M), and ViT-L (335.3M), respectively. Best, second best results.\\ndepth model with an auxiliary feature alignment loss. The\\nfeature space is high-dimensional and continuous, thus con-\\ntaining richer semantic information than discrete masks. The\\nfeature alignment loss is formulated as:\\nLfeat= 1−1\\nHWHWX\\ni=1cos(fi, f′\\ni), (9)\\nwhere cos(·,·)measures the cosine similarity between two\\nfeature vectors. fis the feature extracted by the depth model\\nS, while f′is the feature from a frozen DINOv2 encoder.\\nWe do not follow some works [ 19] to project the online\\nfeature finto a new space for alignment, because a randomly\\ninitialized projector makes the large alignment loss dominate\\nthe overall loss in the early stage.\\nAnother key point in feature alignment is that, semantic\\nencoders like DINOv2 tend to produce similar features for\\ndifferent parts of an object, e.g., car front and rear. In depth\\nestimation, however, different parts or even pixels within the\\nsame part, can be of varying depth. Thus, it is not beneficial\\ntoexhaustively enforce our depth model to produce exactly\\nthe same features as the frozen encoder.\\nTo solve this issue, we set a tolerance margin αfor the\\nfeature alignment. If the cosine similarity of fiandf′\\nihas\\nsurpassed α, this pixel will not be considered in our Lfeat.\\nThis allows our method to enjoy both the semantic-aware\\nrepresentation from DINOv2 and the part-level discrimina-\\ntive representation from depth supervision. As a side effect,\\nour produced encoder not only performs well in downstream\\nMDE datasets, but also achieves strong results in the seman-\\ntic segmentation task. It also indicates the potential of our\\nencoder to serve as a universal multi-task encoder for both\\nmiddle-level and high-level perception tasks.\\nFinally, our overall loss is an average combination of the\\nthree losses Ll,Lu, andLfeat.\\n4. Experiment\\n4.1. Implementation Details\\nWe adopt the DINOv2 encoder [ 43] for feature extraction.\\nFollowing MiDaS [5, 46], we use the DPT [47] decoder fordepth regression. All labeled datasets are simply combined\\ntogether without re-sampling. In the first stage, we train a\\nteacher model on labeled images for 20 epochs. In the second\\nstage of joint training, we train a student model to sweep\\nacross all unlabeled images for one time. The unlabeled\\nimages are annotated by a best-performed teacher model\\nwith a ViT-L encoder. The ratio of labeled and unlabeled\\nimages is set as 1:2 in each batch. In both stages, the base\\nlearning rate of the pre-trained encoder is set as 5e-6, while\\nthe randomly initialized decoder uses a 10 ×larger learning\\nrate. We use the AdamW optimizer and decay the learning\\nrate with a linear schedule. We only apply horizontal flipping\\nas our data augmentation for labeled images. The tolerance\\nmargin αfor feature alignment loss is set as 0.15. For more\\ndetails, please refer to our appendix.\\n4.2. Zero-Shot Relative Depth Estimation\\nAs aforementioned, this work aims to provide accurate\\ndepth estimation for any image. Therefore, we compre-\\nhensively validate the zero-shot depth estimation capability\\nof our Depth Anything model on six representative unseen\\ndatasets: KITTI [ 18], NYUv2 [ 55], Sintel [ 7], DDAD [ 20],\\nETH3D [ 52], and DIODE [ 60]. We compare with the best\\nDPT-BEiT L-512 model from the latest MiDaS v3.1 [ 5], which\\nuses more labeled images than us. As shown in Table 2,\\nboth with a ViT-L encoder, our Depth Anything surpasses\\nthe strongest MiDaS model tremendously across extensive\\nscenes in terms of both the AbsRel (absolute relative error:\\n|d∗−d|/d) and δ1(percentage of max( d∗/d, d/d∗)<1.25)\\nmetrics. For example, when tested on the well-known au-\\ntonomous driving dataset DDAD [ 20], we improve the Ab-\\nsRel (↓) from 0.251 →0.230 and improve the δ1(↑) from\\n0.766→0.789.\\nBesides, our ViT-B model is already clearly superior to\\nthe MiDaS based on a much larger ViT-L. Moreover, our\\nViT-S model, whose scale is less than 1/10 of the MiDaS\\nmodel, even outperforms MiDaS on several unseen datasets,\\nincluding Sintel, DDAD, and ETH3D. The performance\\nadvantage of these small-scale models demonstrates their\\ngreat potential in computationally-constrained scenarios.\\nIt is also worth noting that, on the most widely used MDE\\n5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='45905b40-5de7-47d8-81b6-9aa236c6ad6a', embedding=None, metadata={'page_label': '6', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MethodHigher is better ↑ Lower is better ↓\\nδ1 δ2 δ3 AbsRel RMSE log10\\nAdaBins [3] 0.903 0.984 0.997 0.103 0.364 0.044\\nDPT [47] 0.904 0.988 0.998 0.110 0.357 0.045\\nP3Depth [44] 0.898 0.981 0.996 0.104 0.356 0.043\\nSwinV2-L [40] 0.949 0.994 0.999 0.083 0.287 0.035\\nAiT [42] 0.954 0.994 0.999 0.076 0.275 0.033\\nVPD [87] 0.964 0.995 0.999 0.069 0.254 0.030\\nZoeDepth∗[4] 0.951 0.994 0.999 0.077 0.282 0.033\\nOurs 0.984 0.998 1.000 0.056 0.206 0.024\\nTable 3. Fine-tuning and evaluating on NYUv2 [55] with our\\npre-trained MDE encoder. We highlight best,second best results,\\nas well as most discriminative metrics .∗: Reproduced by us.\\nbenchmarks KITTI and NYUv2, although MiDaS v3.1 uses\\nthe corresponding training images ( not zero-shot anymore ),\\nour Depth Anything is still evidently superior to it without\\ntraining with any KITTI or NYUv2 images ,e.g., 0.127 vs.\\n0.076 in AbsRel and 0.850 vs.0.947 in δ1on KITTI.\\n4.3. Fine-tuned to Metric Depth Estimation\\nApart from the impressive performance in zero-shot relative\\ndepth estimation, we further examine our Depth Anything\\nmodel as a promising weight initialization for downstream\\nmetric depth estimation. We initialize the encoder of down-\\nstream MDE models with our pre-trained encoder parameters\\nand leave the decoder randomly initialized. The model is\\nfine-tuned with correponding metric depth information. In\\nthis part, we use our ViT-L encoder for fine-tuning.\\nWe examine two representative scenarios: 1) in-domain\\nmetric depth estimation, where the model is trained and\\nevaluated on the same domain (Section 4.3.1), and 2) zero-\\nshot metric depth estimation, where the model is trained on\\none domain, e.g., NYUv2 [ 55], but evaluated in different\\ndomains, e.g., SUN RGB-D [57] (Section 4.3.2).\\n4.3.1 In-Domain Metric Depth Estimation\\nAs shown in Table 3 of NYUv2 [ 55], our model outperforms\\nthe previous best method VPD [ 87] remarkably, improving\\ntheδ1(↑) from 0.964 →0.984 and AbsRel ( ↓) from 0.069\\nto 0.056. Similar improvements can be observed in Table 4\\nof the KITTI dataset [ 18]. We improve the δ1(↑) on KITTI\\nfrom 0.978 →0.982. It is worth noting that we adopt the\\nZoeDepth framework for this scenario with a relatively ba-\\nsic depth model, and we believe our results can be further\\nenhanced if equipped with more advanced architectures.\\n4.3.2 Zero-Shot Metric Depth Estimation\\nWe follow ZoeDepth [ 4] to conduct zero-shot metric depth\\nestimation. ZoeDepth fine-tunes the MiDaS pre-trained en-MethodHigher is better ↑ Lower is better ↓\\nδ1 δ2 δ3 AbsRel RMSE RMSE log\\nAdaBins [3] 0.964 0.995 0.999 0.058 2.360 0.088\\nDPT [47] 0.959 0.995 0.999 0.062 2.573 0.092\\nP3Depth [44] 0.953 0.993 0.998 0.071 2.842 0.103\\nNeWCRFs [83] 0.974 0.997 0.999 0.052 2.129 0.079\\nSwinV2-L [40] 0.977 0.998 1.000 0.050 1.966 0.075\\nNDDepth [54] 0.978 0.998 0.999 0.050 2.025 0.075\\nGEDepth [76] 0.976 0.997 0.999 0.048 2.044 0.076\\nZoeDepth∗[4] 0.971 0.996 0.999 0.054 2.281 0.082\\nOurs 0.982 0.998 1.000 0.046 1.896 0.069\\nTable 4. Fine-tuning and evaluating on KITTI [18] with our\\npre-trained MDE encoder. ∗: Reproduced by us.\\ncoder with metric depth information from NYUv2 [ 55] (for\\nindoor scenes) or KITTI [ 18] (for outdoor scenes). There-\\nfore, we simply replace the MiDaS encoder with our bet-\\nter Depth Anything encoder, leaving other components un-\\nchanged. As shown in Table 5, across a wide range of unseen\\ndatasets of indoor and outdoor scenes, our Depth Anything\\nresults in a better metric depth estimation model than the\\noriginal ZoeDepth based on MiDaS.\\n4.4. Fine-tuned to Semantic Segmentation\\nIn our method, we design our MDE model to inherit the\\nrich semantic priors from a pre-trained encoder via a sim-\\nple feature alignment constraint. Here, we examine the\\nsemantic capability of our MDE encoder. Specifically, we\\nfine-tune our MDE encoder to downstream semantic segmen-\\ntation datasets. As exhibited in Table 7 of the Cityscapes\\ndataset [ 15], our encoder from large-scale MDE training\\n(86.2 mIoU) is superior to existing encoders from large-scale\\nImageNet-21K pre-training, e.g., Swin-L [ 39] (84.3) and\\nConvNeXt-XL [ 41] (84.6). Similar observations hold on the\\nADE20K dataset [ 89] in Table 8. We improve the previous\\nbest result from 58.3 →59.4.\\nWe hope to highlight that, witnessing the superiority of\\nour pre-trained encoder on both monocular depth estimation\\nand semantic segmentation tasks, we believe it has great\\npotential to serve as a generic multi-task encoder for both\\nmiddle-level and high-level visual perception systems.\\n4.5. Ablation Studies\\nUnless otherwise specified, we use the ViT-L encoder for\\nour ablation studies here.\\nZero-shot transferring of each training dataset. In Ta-\\nble 6, we provide the zero-shot transferring performance of\\neach training dataset, which means that we train a relative\\nMDE model on onetraining set and evaluate it on the six\\nunseen datasets. With these results, we hope to offer more\\ninsights for future works that similarly aim to build a general\\n6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e0447fe5-cbfc-41ab-8329-54dea200614d', embedding=None, metadata={'page_label': '7', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='MethodSUN RGB-D [57] iBims-1 [29] HyperSim [49] Virtual KITTI 2 [8] DIODE Outdoor [60]\\nAbsRel ( ↓)δ1(↑) AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1\\nZoeDepth [4] 0.520 0.545 0.169 0.656 0.407 0.302 0.106 0.844 0.814 0.237\\nDepth Anything 0.500 0.660 0.150 0.714 0.363 0.361 0.085 0.913 0.794 0.288\\nTable 5. Zero-shot metric depth estimation. The first three test sets in the header are indoor scenes, while the last two are outdoor scenes.\\nFollowing ZoeDepth, we use the model trained on NYUv2 for indoor generalization, while use the model trained on KITTI for outdoor\\nevaluation. For fair comparisons, we report the ZoeDepth results reproduced in our environment.\\nTraining setKITTI [18] NYUv2 [55] Sintel [7] DDAD [20] ETH3D [52] DIODE [60] Mean\\nAbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1AbsRel δ1\\nBlendedMVS [77] 0.089 0.918 0.068 0.958 0.556 0.689 0.305 0.731 0.148 0.845 0.092 0.921 0.210 0.844\\nDIML [13] 0.099 0.907 0.055 0.969 0.573 0.722 0.381 0.657 0.142 0.859 0.107 0.908 0.226 0.837\\nHRWSI [68] 0.095 0.917 0.062 0.966 0.502 0.731 0.270 0.750 0.186 0.775 0.087 0.935 0.200 0.846\\nIRS [62] 0.105 0.892 0.057 0.970 0.568 0.714 0.328 0.691 0.143 0.845 0.088 0.926 0.215 0.840\\nMegaDepth [33] 0.217 0.741 0.071 0.953 0.632 0.660 0.479 0.566 0.142 0.852 0.104 0.910 0.274 0.780\\nTartanAir [63] 0.088 0.920 0.061 0.964 0.602 0.723 0.332 0.690 0.160 0.818 0.088 0.928 0.222 0.841\\nAll labeled data 0.085 0.934 0.053 0.971 0.492 0.748 0.245 0.771 0.134 0.874 0.070 0.945 0.180 0.874\\nTable 6. Examine the zero-shot transferring performance of each labeled training set (left) to six unseen datasets (top). Better performance:\\nAbsRel ↓,δ1↑. We highlight the best, second , and third best results for each test dataset in bold , underline , and italic , respectively.\\nMethod Encoder mIoU (s.s.) m.s.\\nSegmenter [58] ViT-L [16] - 82.2\\nSegFormer [70] MiT-B5 [70] 82.4 84.0\\nMask2Former [12] Swin-L [39] 83.3 84.3\\nOneFormer [24] Swin-L [39] 83.0 84.4\\nOneFormer [24] ConvNeXt-XL [41] 83.6 84.6\\nDDP [25] ConvNeXt-L [41] 83.2 83.9\\nOurs ViT-L [16] 84.8 86.2\\nTable 7. Transferring our MDE pre-trained encoder to Cityscapes\\nfor semantic segmentation. We do not use Mapillary [ 1] for pre-\\ntraining. s.s./m.s.: single-/multi-scale evaluation.\\nmonocular depth estimation system. Among the six training\\ndatasets, HRWSI [ 68] fuels our model with the strongest\\ngeneralization ability, even though it only contains 20K im-\\nages. This indicates the data diversity counts a lot, which\\nis well aligned with our motivation to utilize unlabeled im-\\nages. Some labeled datasets may not perform very well, e.g.,\\nMegaDepth [ 33], however, it has its own preferences that\\nare not reflected in these six test datasets. For example, we\\nfind models trained with MegaDepth data are specialized at\\nestimating the distance of ultra-remote buildings (Figure 1),\\nwhich will be very beneficial for aerial vehicles.\\nEffectiveness of 1) challenging the student model when\\nlearning unlabeled images, and 2) semantic constraint.\\nAs shown in Table 9, simply adding unlabeled images with\\npseudo labels does not necessarily bring gains to our model,Method Encoder mIoU\\nSegmenter [58] ViT-L [16] 51.8\\nSegFormer [70] MiT-B5 [70] 51.0\\nMask2Former [12] Swin-L [39] 56.4\\nUperNet [69] BEiT-L [2] 56.3\\nViT-Adapter [11] BEiT-L [2] 58.3\\nOneFormer [24] Swin-L [39] 57.4\\nOneFormer [24] ConNeXt-XL [41] 57.4\\nOurs ViT-L [16] 59.4\\nTable 8. Transferring our MDE encoder to ADE20K for semantic\\nsegmentation. We use Mask2Former as our segmentation model.\\nsince the labeled images are already sufficient. However,\\nwith strong perturbations ( S) applied to unlabeled images\\nduring re-training, the student model is challenged to seek\\nadditional visual knowledge and learn more robust repre-\\nsentations. Consequently, the large-scale unlabeled images\\nenhance the model generalization ability significantly.\\nMoreover, with our used semantic constraint Lfeat, the\\npower of unlabeled images can be further amplified for the\\ndepth estimation task. More importantly, as emphasized in\\nSection 4.4, this auxiliary constraint also enables our trained\\nencoder to serve as a key component in a multi-task visual\\nsystem for both middle-level and high-level perception.\\nComparison with MiDaS trained encoder in downstream\\ntasks. Our Depth Anything model has exhibited stronger\\nzero-shot capability than MiDaS [ 5,46]. Here, we further\\n7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a080768e-fa03-43b1-b30f-e4817c94a75e', embedding=None, metadata={'page_label': '8', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 3. Qualitative results on six unseen datasets.\\nLlLuS L feat KI NY SI DD ET DI\\n✓ 0.085 0.053 0.492 0.245 0.134 0.070\\n✓ ✓ 0.085 0.054 0.481 0.242 0.138 0.073\\n✓ ✓ ✓ 0.081 0.048 0.469 0.235 0.134 0.068\\n✓ ✓ ✓ ✓ 0.076 0.043 0.458 0.230 0.127 0.066\\nTable 9. Ablation studies of: 1) challenging the student with strong\\nperturbations ( S) when learning unlabeled images, and 2) semantic\\nconstraint ( Lfeat). Limited by space, we only report the AbsRel\\n(↓) metric, and shorten the dataset name with its first two letters.\\nMethodNYUv2 KITTI Cityscapes ADE20K\\nAbsRel δ1AbsRel δ1 mIoU mIoU\\nMiDaS 0.077 0.951 0.054 0.971 82.1 52.4\\nOurs 0.056 0.984 0.046 0.982 84.8 59.4\\nTable 10. Comparison between our trained encoder and MiDaS [ 5]\\ntrained encoder in terms of downstream fine-tuning performance.\\nBetter performance: AbsRel ↓,δ1↑, mIoU ↑.\\ncompare our trained encoder with MiDaS v3.1 [ 5] trained\\nencoder in terms of the downstream fine-tuning performance.\\nAs demonstrated in Table 10, on both the downstream depth\\nestimation task and semantic segmentation task, our pro-\\nduced encoder outperforms the MiDaS encoder remarkably,\\ne.g., 0.951 vs.0.984 in the δ1metric on NYUv2, and 52.4\\nvs.59.4 in the mIoU metric on ADE20K.\\nComparison with DINOv2 in downstream tasks. We have\\ndemonstrated the superiority of our trained encoder when\\nfine-tuned to downstream tasks. Since our finally produced\\nencoder (from large-scale MDE training) is fine-tuned from\\nDINOv2 [ 43], we compare our encoder with the original\\nDINOv2 encoder in Table 11. It can be observed that our\\nencoder performs better than the original DINOv2 encoder\\nin both the downstream metric depth estimation task and\\nsemantic segmentation task. Although the DINOv2 weight\\nhas provided a very strong initialization (also much better\\nthan the MiDaS encoder as reported in Table 10), our large-\\nscale and high-quality MDE training can further enhance it\\nOurs\\n MiDaS Ours MiDaS\\nFigure 4. We compare our depth prediction with MiDaS. Meantime,\\nwe use ControlNet to synthesize new images from the depth map\\n(the last row). First row: input image, second row: depth prediction.\\nEncoderNYUv2 KITTI ADE20K\\nAbsRel ( ↓)δ1(↑) AbsRel δ1 mIoU ( ↑)\\nDINOv2 0.066 0.973 0.058 0.971 58.8\\nOurs 0.056 0.984 0.046 0.982 59.4\\nTable 11. Comparison between the original DINOv2 and our pro-\\nduced encoder in terms of downstream fine-tuning performance.\\nimpressively in downstream transferring performance.\\n4.6. Qualitative Results\\nWe visualize our model predictions on the six unseen datasets\\nin Figure 3. Our model is robust to test images from various\\ndomains. In addition, we compare our model with MiDaS\\nin Figure 4. We also attempt to synthesis new images con-\\nditioned on the predicted depth maps with ControlNet [ 85].\\nOur model produces more accurate depth estimation than\\nMiDaS, as well as better synthesis results, although the Con-\\ntrolNet is trained with MiDaS depth. For more accurate\\nsynthesis, we have also re-trained a better depth-conditioned\\nControlNet based on our Depth Anything, aiming to provide\\nbetter control signals for image synthesis and video editing.\\nPlease refer to our project page for more qualitative results\\non video editing [35] with our Depth Anything.\\n5. Conclusion\\nIn this work, we present Depth Anything, a highly practical\\nsolution to robust monocular depth estimation. Different\\nfrom prior arts, we especially highlight the value of cheap\\nand diverse unlabeled images. We design two simple yet\\nhighly effective strategies to fully exploit their value: 1)\\nposing a more challenging optimization target when learning\\nunlabeled images, and 2) preserving rich semantic priors\\nfrom pre-trained models. As a result, our Depth Anything\\nmodel exhibits excellent zero-shot depth estimation ability,\\nand also serves as a promising initialization for downstream\\nmetric depth estimation and semantic segmentation tasks.\\n8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d26da22a-8732-44c6-bf7a-dad628438ae7', embedding=None, metadata={'page_label': '9', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data\\nSupplementary Material\\n6. More Implementation Details\\nWe resize the shorter side of all images to 518 and keep the\\noriginal aspect ratio. All images are cropped to 518 ×518\\nduring training. During inference, we do not crop images\\nand only ensure both sides are multipliers of 14, since the\\npre-defined patch size of DINOv2 encoders [ 43] is 14. Eval-\\nuation is performed at the original resolution by interpolating\\nthe prediction. Following MiDaS [ 5,46], in zero-shot eval-\\nuation, the scale and shift of our prediction are manually\\naligned with the ground truth.\\nWhen fine-tuning our pre-trained encoder to metric depth\\nestimation, we adopt the ZoeDepth codebase [ 4]. We merely\\nreplace the original MiDaS-based encoder with our stronger\\nDepth Anything encoder, with a few hyper-parameters mod-\\nified. Concretely, the training resolution is 392 ×518 on\\nNYUv2 [ 55] and 384 ×768 on KITTI [ 18] to match the\\npatch size of our encoder. The encoder learning rate is\\nset as 1/50 of the learning rate of the randomly initialized\\ndecoder, which is much smaller than the 1/10 adopted for\\nMiDaS encoder, due to our strong initialization. The batch\\nsize is 16 and the model is trained for 5 epochs.\\nWhen fine-tuning our pre-trained encoder to semantic seg-\\nmentation, we use the MMSegmentation codebase [ 14]. The\\ntraining resolution is set as 896 ×896 on both ADE20K [ 89]\\nand Cityscapes [ 15]. The encoder learning rate is set as\\n3e-6 and the decoder learning rate is 10 ×larger. We use\\nMask2Former [ 12] as our semantic segmentation model. The\\nmodel is trained for 160K iterations on ADE20K and 80K\\niterations on Cityscapes both with batch size 16, without\\nany COCO [ 36] or Mapillary [ 1] pre-training. Other training\\nconfigurations are the same as the original codebase.\\n7. More Ablation Studies\\nAll ablation studies here are conducted on the ViT-S model.\\nThe necessity of tolerance margin for feature alignment.\\nAs shown in Table 12, the gap between the tolerance margin\\nof 0 and 0.15 or 0.30 clearly demonstrates the necessity of\\nthis design (mean AbsRel: 0.188 vs.0.175).\\nApplying feature alignment to labeled data. Previously,\\nwe enforce the feature alignment loss Lfeat on unlabeled\\ndata. Indeed, it is technically feasible to also apply this\\nconstraint to labeled data. In Table 13, apart from applying\\nLfeat on unlabeled data, we explore to apply it to labeled\\ndata. We find that adding this auxiliary optimization target\\nto labeled data is not beneficial to our baseline that does not\\ninvolve any feature alignment (their mean AbsRel values are\\nalmost the same: 0.180 vs.0.179). We conjecture that this isα KITTI NYU Sintel DDAD ETH3D DIODE Mean\\n0.00 0.085 0.055 0.523 0.250 0.134 0.079 0.188\\n0.15 0.080 0.053 0.464 0.247 0.127 0.076 0.175\\n0.30 0.079 0.054 0.482 0.248 0.127 0.077 0.178\\nTable 12. Ablation studies on different values of the tolerance\\nmargin αfor the feature alignment loss Lfeat. Limited by space,\\nwe only report the AbsRel ( ↓) metric here.\\nLfeat Unseen datasets (AbsRel ↓)Mean\\nU L KITTI NYU Sintel DDAD ETH3D DIODE\\n0.083 0.055 0.478 0.249 0.133 0.080 0.180\\n✓ 0.080 0.053 0.464 0.247 0.127 0.076 0.175\\n✓0.084 0.054 0.472 0.252 0.133 0.081 0.179\\nTable 13. Ablation studies of applying our feature alignment loss\\nLfeat to unlabeled data ( U) or labeled data ( L).\\nbecause the labeled data has relatively higher-quality depth\\nannotations. The involvement of semantic loss may interfere\\nwith the learning of these informative manual labels. In com-\\nparison, our pseudo labels are noisier and less informative.\\nTherefore, introducing the auxiliary constraint to unlabeled\\ndata can combat the noise in pseudo depth labels, as well as\\narm our model with semantic capability.\\n8. Limitations and Future Works\\nCurrently, the largest model size is only constrained to ViT-\\nLarge [ 16]. Therefore, in the future, we plan to further scale\\nup the model size from ViT-Large to ViT-Giant, which is\\nalso well pre-trained by DINOv2 [43]. We can train a more\\npowerful teacher model with the larger model, producing\\nmore accurate pseudo labels for smaller models to learn, e.g.,\\nViT-L and ViT-B. Furthermore, to facilitate real-world ap-\\nplications, we believe the widely adopted 512 ×512 training\\nresolution is not enough. We plan to re-train our model on a\\nlarger resolution of 700+ or even 1000+.\\n9. More Qualitative Results\\nPlease refer to the following pages for comprehensive quali-\\ntative results on six unseen test sets (Figure 5 for KITTI [ 18],\\nFigure 6 for NYUv2 [ 55], Figure 7 for Sintel [ 7], Figure 8\\nfor DDAD [ 20], Figure 9 for ETH3D [ 52], and Figure 10\\nfor DIODE [ 60]). We compare our model with the strongest\\nMiDaS model [ 5],i.e., DPT-BEiT L-512. Our model exhibits\\nhigher depth estimation accuracy and stronger robustness.\\n9', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7088d018-0359-4406-947a-f38962421fae', embedding=None, metadata={'page_label': '10', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input image Our prediction MiDaS v3.1 prediction\\nFigure 5. Qualitative results on KITTI. Due to the extremely sparse ground truth which is hard to visualize, we here compare our prediction\\nwith the most advanced MiDaS v3.1 [5] prediction. The brighter color denotes the closer distance.\\n10', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09ef6690-e1ab-4728-b9e2-00999d21b5c1', embedding=None, metadata={'page_label': '11', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input image Our prediction MiDaS v3.1 prediction\\nFigure 6. Qualitative results on NYUv2. It is worth noting that MiDaS [5] uses NYUv2 training data ( not zero-shot ), while we do not.\\n11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e6fa384-42ed-42fc-b991-ece3a8873fe4', embedding=None, metadata={'page_label': '12', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input image Our prediction MiDaS v3.1 prediction\\nFigure 7. Qualitative results on Sintel.\\n12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e56df7be-0cc3-42f8-ae3a-dbd7a12be802', embedding=None, metadata={'page_label': '13', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input image Our prediction MiDaS v3.1 prediction\\n Input image Our prediction MiDaS v3.1 prediction\\nFigure 8. Qualitative results on DDAD.\\n13', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f85a1d19-b85b-4dc6-80d8-f746b775c08e', embedding=None, metadata={'page_label': '14', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input image Our prediction MiDaS v3.1 predictionFigure 9. Qualitative results on ETH3D.\\n14', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0abb6ac9-c55c-472b-b770-390b8da6b33e', embedding=None, metadata={'page_label': '15', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Input image Our prediction MiDaS v3.1 predictionFigure 10. Qualitative results on DIODE.\\n15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='74a8b7af-6c15-4f0b-8280-c4902f1c111f', embedding=None, metadata={'page_label': '16', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\n[1]Manuel L ´opez Antequera, Pau Gargallo, Markus Hofinger,\\nSamuel Rota Bul `o, Yubin Kuang, and Peter Kontschieder.\\nMapillary planet-scale depth dataset. In ECCV , 2020. 7, 9\\n[2]Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\\nBert pre-training of image transformers. In ICLR , 2022. 7\\n[3]Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka.\\nAdabins: Depth estimation using adaptive bins. In CVPR ,\\n2021. 2, 6\\n[4]Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka,\\nand Matthias M ¨uller. Zoedepth: Zero-shot transfer by com-\\nbining relative and metric depth. arXiv:2302.12288 , 2023. 2,\\n6, 7, 9\\n[5]Reiner Birkl, Diana Wofk, and Matthias M ¨uller. Midas v3.\\n1–a model zoo for robust monocular relative depth estimation.\\narXiv:2307.14460 , 2023. 2, 3, 5, 7, 8, 9, 10, 11\\n[6]Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\\nAltman, Simran Arora, Sydney von Arx, Michael S Bern-\\nstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,\\net al. On the opportunities and risks of foundation models.\\narXiv:2108.07258 , 2021. 1\\n[7]Daniel J Butler, Jonas Wulff, Garrett B Stanley, and Michael J\\nBlack. A naturalistic open source movie for optical flow\\nevaluation. In ECCV , 2012. 5, 7, 9\\n[8]Yohann Cabon, Naila Murray, and Martin Humenberger. Vir-\\ntual kitti 2. arXiv:2001.10773 , 2020. 7\\n[9]Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-\\nChiang Frank Wang. Towards scene understanding: Un-\\nsupervised monocular depth estimation with semantic-aware\\nrepresentation. In CVPR , 2019. 2, 4\\n[10] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\\nimage depth perception in the wild. In NeurIPS , 2016. 2\\n[11] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong\\nLu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for\\ndense predictions. In ICLR , 2023. 7\\n[12] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander\\nKirillov, and Rohit Girdhar. Masked-attention mask trans-\\nformer for universal image segmentation. In CVPR , 2022. 7,\\n9\\n[13] Jaehoon Cho, Dongbo Min, Youngjung Kim, and Kwanghoon\\nSohn. Diml/cvl rgb-d dataset: 2m rgb-d images of natural\\nindoor and outdoor scenes. arXiv:2110.11590 , 2021. 3, 7\\n[14] MMSegmentation Contributors. MMSegmenta-\\ntion: Openmmlab semantic segmentation toolbox\\nand benchmark. https : / / github . com / open -\\nmmlab/mmsegmentation , 2020. 9\\n[15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\\nStefan Roth, and Bernt Schiele. The cityscapes dataset for\\nsemantic urban scene understanding. In CVPR , 2016. 1, 6, 9\\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. In ICLR , 2021. 7,\\n9[17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth\\nmap prediction from a single image using a multi-scale deep\\nnetwork. In NeurIPS , 2014. 2\\n[18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\\nUrtasun. Vision meets robotics: The kitti dataset. IJRR , 2013.\\n1, 2, 3, 5, 6, 7, 9\\n[19] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin\\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-\\nsch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\\nlaghi Azar, et al. Bootstrap your own latent-a new approach\\nto self-supervised learning. In NeurIPS , 2020. 5\\n[20] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos,\\nand Adrien Gaidon. 3d packing for self-supervised monocular\\ndepth estimation. In CVPR , 2020. 5, 7, 9\\n[21] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien\\nGaidon. Semantically-guided representation learning for self-\\nsupervised monocular depth. In ICLR , 2020. 2, 4\\n[22] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rare s,Ambru s,,\\nand Adrien Gaidon. Towards zero-shot scale-aware monocu-\\nlar depth estimation. In ICCV , 2023. 2\\n[23] Derek Hoiem, Alexei A Efros, and Martial Hebert. Recover-\\ning surface layout from an image. IJCV , 2007. 2\\n[24] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita\\nOrlov, and Humphrey Shi. Oneformer: One transformer to\\nrule universal image segmentation. In CVPR , 2023. 7\\n[25] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,\\nZhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp:\\nDiffusion model for dense visual prediction. In ICCV , 2023.\\n7\\n[26] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing\\nTai, Chi-Keung Tang, and Fisher Yu. Segment anything in\\nhigh quality. In NeurIPS , 2023. 4\\n[27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\\nthing. In ICCV , 2023. 1, 2, 3\\n[28] Marvin Klingner, Jan-Aike Term ¨ohlen, Jonas Mikolajczyk,\\nand Tim Fingscheidt. Self-supervised monocular depth es-\\ntimation: Solving the dynamic object problem by semantic\\nguidance. In ECCV , 2020. 4\\n[29] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco\\nKorner. Evaluation of cnn-based single-image depth estima-\\ntion methods. In ECCVW , 2018. 7\\n[30] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,\\nIvan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,\\nMatteo Malloci, Alexander Kolesnikov, et al. The open im-\\nages dataset v4: Unified image classification, object detection,\\nand visual relationship detection at scale. IJCV , 2020. 2, 3\\n[31] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient\\nsemi-supervised learning method for deep neural networks.\\nInICMLW , 2013. 2\\n[32] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hen-\\ngel, and Mingyi He. Depth and surface normal estimation\\nfrom monocular images using regression on deep features and\\nhierarchical crfs. In CVPR , 2015. 2\\n[33] Zhengqi Li and Noah Snavely. Megadepth: Learning single-\\nview depth prediction from internet photos. In CVPR , 2018.\\n1, 3, 7\\n16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4df2684-025e-49bf-bd61-544468fa778a', embedding=None, metadata={'page_label': '17', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[34] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.\\nBinsformer: Revisiting adaptive bins for monocular depth\\nestimation. arXiv:2204.00987 , 2022. 2\\n[35] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,\\nand Jiashi Feng. Magicedit: High-fidelity and temporally\\ncoherent video editing. arXiv:2308.14749 , 2023. 8\\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\\nPietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence\\nZitnick. Microsoft coco: Common objects in context. In\\nECCV , 2014. 1, 9\\n[37] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and\\nWilliam T Freeman. Sift flow: Dense correspondence across\\ndifferent scenes. In ECCV , 2008. 2\\n[38] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\\nZhu, et al. Grounding dino: Marrying dino with grounded\\npre-training for open-set object detection. arXiv:2303.05499 ,\\n2023. 4\\n[39] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\\nHierarchical vision transformer using shifted windows. In\\nICCV , 2021. 6, 7\\n[40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.\\nSwin transformer v2: Scaling up capacity and resolution. In\\nCVPR , 2022. 6\\n[41] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\\n2020s. In CVPR , 2022. 6, 7\\n[42] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang\\nGeng, Qi Dai, Kun He, and Han Hu. All in tokens: Unifying\\noutput space of visual tasks via soft token. In ICCV , 2023. 6\\n[43] Maxime Oquab, Timoth ´ee Darcet, Th ´eo Moutakanni, Huy V o,\\nMarc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel\\nHaziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2:\\nLearning robust visual features without supervision. TMLR ,\\n2023. 3, 4, 5, 8, 9\\n[44] Vaishakh Patil, Christos Sakaridis, Alexander Liniger, and\\nLuc Van Gool. P3depth: Monocular depth estimation with a\\npiecewise planarity prior. In CVPR , 2022. 6\\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervision.\\nInICML , 2021. 1\\n[46] Ren´e Ranftl, Katrin Lasinger, David Hafner, Konrad\\nSchindler, and Vladlen Koltun. Towards robust monocular\\ndepth estimation: Mixing datasets for zero-shot cross-dataset\\ntransfer. TPAMI , 2020. 1, 2, 3, 5, 7, 9\\n[47] Ren´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\\nsion transformers for dense prediction. In ICCV , 2021. 5,\\n6\\n[48] Alex Rasla and Michael Beyeler. The relative importance\\nof depth cues and semantic edges for indoor mobility using\\nsimulated prosthetic vision in immersive virtual reality. In\\nVRST , 2022. 1[49] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit\\nKumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb,\\nand Joshua M Susskind. Hypersim: A photorealistic synthetic\\ndataset for holistic indoor scene understanding. In ICCV ,\\n2021. 7\\n[50] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, et al. Imagenet large scale\\nvisual recognition challenge. IJCV , 2015. 3\\n[51] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:\\nLearning 3d scene structure from a single still image. TPAMI ,\\n2008. 2\\n[52] Thomas Schops, Johannes L Schonberger, Silvano Galliani,\\nTorsten Sattler, Konrad Schindler, Marc Pollefeys, and An-\\ndreas Geiger. A multi-view stereo benchmark with high-\\nresolution images and multi-camera videos. In CVPR , 2017.\\n5, 7, 9\\n[53] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\\nlarge-scale, high-quality dataset for object detection. In ICCV ,\\n2019. 3\\n[54] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and\\nZhengguo Li. Nddepth: Normal-distance assisted monocular\\ndepth estimation. In ICCV , 2023. 2, 6\\n[55] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob\\nFergus. Indoor segmentation and support inference from rgbd\\nimages. In ECCV , 2012. 1, 2, 3, 5, 6, 7, 9\\n[56] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,\\nAlexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying\\nsemi-supervised learning with consistency and confidence. In\\nNeurIPS , 2020. 2, 4\\n[57] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.\\nSun rgb-d: A rgb-d scene understanding benchmark suite. In\\nCVPR , 2015. 6, 7\\n[58] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\\nSchmid. Segmenter: Transformer for semantic segmentation.\\nInICCV , 2021. 7\\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\\ntinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste\\nRozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\\nLlama: Open and efficient foundation language models.\\narXiv:2302.13971 , 2023. 1\\n[60] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,\\nHaochen Wang, Falcon Z Dai, Andrea F Daniele, Mo-\\nhammadreza Mostajabi, Steven Basart, Matthew R Walter,\\net al. Diode: A dense indoor and outdoor depth dataset.\\narXiv:1908.00463 , 2019. 5, 7, 9\\n[61] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver\\nWang. Web stereo video supervision for depth prediction\\nfrom dynamic scenes. In 3DV, 2019. 3\\n[62] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiy-\\nong Zhao, and Xiaowen Chu. Irs: A large naturalistic indoor\\nrobotics stereo dataset to train deep models for disparity and\\nsurface normal estimation. In ICME , 2021. 3, 7\\n[63] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu,\\nYuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and\\n17', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='867cba16-2528-41ba-9b98-2dd6da03bbe5', embedding=None, metadata={'page_label': '18', 'file_name': 'paper (1).pdf', 'file_path': '/content/Data/paper (1).pdf', 'file_type': 'application/pdf', 'file_size': 4549655, 'creation_date': '2024-02-01', 'last_modified_date': '2024-02-01', 'last_accessed_date': '2024-02-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Sebastian Scherer. Tartanair: A dataset to push the limits of\\nvisual slam. In IROS , 2020. 3, 7\\n[64] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariha-\\nran, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar\\nfrom visual depth estimation: Bridging the gap in 3d object\\ndetection for autonomous driving. In CVPR , 2019. 1\\n[65] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle landmarks dataset v2-a large-scale benchmark for\\ninstance-level recognition and retrieval. In CVPR , 2020. 3\\n[66] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman,\\nand Vivienne Sze. Fastdepth: Fast monocular depth estima-\\ntion on embedded systems. In ICRA , 2019. 1\\n[67] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao,\\nRuibo Li, and Zhenbo Luo. Monocular relative depth per-\\nception with web stereo data supervision. In CVPR , 2018. 2,\\n3\\n[68] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin,\\nand Zhiguo Cao. Structure-guided ranking loss for single\\nimage depth prediction. In CVPR , 2020. 2, 3, 7\\n[69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\\nJian Sun. Unified perceptual parsing for scene understanding.\\nInECCV , 2018. 7\\n[70] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\\nJose M Alvarez, and Ping Luo. Segformer: Simple and\\nefficient design for semantic segmentation with transformers.\\nInNeurIPS , 2021. 3, 7\\n[71] Mengde Xu, Zheng Zhang, Han Hu, Jianfeng Wang, Lijuan\\nWang, Fangyun Wei, Xiang Bai, and Zicheng Liu. End-to-end\\nsemi-supervised object detection with soft teacher. In ICCV ,\\n2021. 2\\n[72] Xiaogang Xu, Hengshuang Zhao, Vibhav Vineet, Ser-Nam\\nLim, and Antonio Torralba. Mtformer: Multi-task learning\\nvia transformer and cross-task reasoning. In ECCV , 2022. 4\\n[73] I Zeki Yalniz, Herv ´e J´egou, Kan Chen, Manohar Paluri, and\\nDhruv Mahajan. Billion-scale semi-supervised learning for\\nimage classification. arXiv:1905.00546 , 2019. 2\\n[74] Lihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao.\\nSt++: Make self-training work better for semi-supervised\\nsemantic segmentation. In CVPR , 2022. 4\\n[75] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and\\nYinghuan Shi. Revisiting weak-to-strong consistency in semi-\\nsupervised semantic segmentation. In CVPR , 2023. 2\\n[76] Xiaodong Yang, Zhuang Ma, Zhiyu Ji, and Zhe Ren. Gedepth:\\nGround embedding for monocular depth estimation. In ICCV ,\\n2023. 2, 6\\n[77] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\\nLei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-\\nscale dataset for generalized multi-view stereo networks. In\\nCVPR , 2020. 3, 7\\n[78] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. En-\\nforcing geometric constraints of virtual normal for depth pre-\\ndiction. In ICCV , 2019. 2\\n[79] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaix-\\nuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:\\nTowards zero-shot metric 3d prediction from a single image.\\nInICCV , 2023. 2[80] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Ge-\\noff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Q\\nWeinberger. Pseudo-lidar++: Accurate depth for 3d object\\ndetection in autonomous driving. In ICLR , 2020. 1\\n[81] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\\nlarge-scale image dataset using deep learning with humans in\\nthe loop. arXiv:1506.03365 , 2015. 3\\n[82] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\\nrell. Bdd100k: A diverse driving dataset for heterogeneous\\nmultitask learning. In CVPR , 2020. 2, 3\\n[83] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and\\nPing Tan. New crfs: Neural window fully-connected crfs for\\nmonocular depth estimation. arXiv:2203.01502 , 2022. 2, 6\\n[84] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\\nlarization strategy to train strong classifiers with localizable\\nfeatures. In ICCV , 2019. 4\\n[85] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\\nconditional control to text-to-image diffusion models. In\\nICCV , 2023. 8\\n[86] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,\\nZhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian\\nLi, Shilong Liu, et al. Recognize anything: A strong image\\ntagging model. arXiv:2306.03514 , 2023. 4\\n[87] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie\\nZhou, and Jiwen Lu. Unleashing text-to-image diffusion\\nmodels for visual perception. In ICCV , 2023. 6\\n[88] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\\nand Antonio Torralba. Places: A 10 million image database\\nfor scene recognition. TPAMI , 2017. 3\\n[89] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\\nriuso, and Antonio Torralba. Scene parsing through ade20k\\ndataset. In CVPR , 2017. 6, 9\\n[90] Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx-\\niao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre-\\ntraining and self-training. In NeurIPS , 2020. 2\\n18', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a QA assistant. Your goal is to answer the questions as accurately as possible based on the instructions and context provided\n",
        "\"\"\"\n",
        "\n",
        "#supported by llama2\n",
        "template =\"<|USER|>{query_str}<|ASSISTANT|>\"\n",
        "query_wrapper_prompt = SimpleInputPrompt(template = template)"
      ],
      "metadata": {
        "id": "15BmSbtUBxog"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIKmlVfkCrZt",
        "outputId": "862bc274-e8b3-44cc-dd03-96ad422ef272"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "5X4YapIzC2bY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    generate_kwargs={\n",
        "        \"temperature\":0.1,\n",
        "        \"do_sample\":False\n",
        "    },\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name = \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map = \"auto\",\n",
        "\n",
        "    model_kwargs = {\n",
        "        \"torch_dtype\" : torch.float16,\n",
        "        \"load_in_8bit\": True\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "5642ba05d71b4e97a1b78a15e815a8b1",
            "9537568d44f04370b5f19aafaf9e58c5",
            "a6eaf2c9f5dd4673952f12dc36afadfc",
            "ca8f85231ab546d0a1853728b5688fef",
            "0d453746e8324f29bcb9b6cf0970ab15",
            "0fbc959da6e948a9ac01f4ea069f03b0",
            "48f36a40ec6f45aab9dfba5f97cf97f6",
            "c976b9f0c56b49eebcf3e8c3454cab27",
            "9e6ac9d996594b4db51be94839e7d90d",
            "01267cb8a49343f3af05d98edf4b19fc",
            "7f4ce1e415524bf8b893c27db6d888f2"
          ]
        },
        "id": "z4Yr8HNdEcgd",
        "outputId": "eff57fac-937f-4689-ebd7-ce2b7e376482"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5642ba05d71b4e97a1b78a15e815a8b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding"
      ],
      "metadata": {
        "id": "-yxqgAITGSoO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
        ")"
      ],
      "metadata": {
        "id": "kJx5O1wiMsEC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    chunk_size = 1024,\n",
        "    llm=llm,\n",
        "    embed_model = embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "yNCO8VWDMzMq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhPPifBZ8wLO",
        "outputId": "f159e320-599f-443d-d522-041b2ff28f41"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a8078775630>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a8078775630>, id_func=<function default_id_func at 0x7a812f797910>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7a8078e1fd60>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a8078775630>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents, service_context = service_context)"
      ],
      "metadata": {
        "id": "Hcvoionf825S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "dk8G43sw9Mpd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resp = query_engine.query(\"\"\"NotImplementedError                       Traceback (most recent call last)\n",
        "<ipython-input-31-9e3ca1cb8736> in <cell line: 1>()\n",
        "----> 1 get_ipython().system('pip install pypdf')\n",
        "      2 get_ipython().system('pip install -q transformers einops accelerate langchain bitsandbytes datasets')\n",
        "\n",
        "2 frames\n",
        "/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py in _run_command(cmd, clear_streamed_output)\n",
        "    166     locale_encoding = locale.getpreferredencoding()\n",
        "    167     if locale_encoding != _ENCODING:\n",
        "--> 168       raise NotImplementedError(\n",
        "    169           'A UTF-8 locale is required. Got {}'.format(locale_encoding)\n",
        "    170       )\n",
        "\n",
        "NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\") \\n\\n what is this error\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS8CsdYm9S2V",
        "outputId": "ebf27ec3-1eb2-406c-816a-318793615bf9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmMwGM9a9Zqz",
        "outputId": "d2e704c9-6cc8-4917-b86a-82eff9f8a16d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The error message you encountered is a NotImplementedError, which means that the code you are trying to execute is not implemented or available in the current version of Google Colab.\n",
            "\n",
            "The error message specifically states that a UTF-8 locale is required, but the current locale is not supported. This could be due to a variety of reasons, such as the version of Python you are using or the dependencies installed in your environment.\n",
            "\n",
            "To resolve this issue, you can try updating your Python version or installing additional dependencies that support UTF-8 encoding. You can also try using a different environment or virtual environment to isolate the issue.\n",
            "\n",
            "In general, it's important to ensure that your environment is set up correctly and that you have the necessary dependencies installed to run the code you are trying to execute. If you are still encountering issues, you can try searching for more detailed solutions or seeking help from the Google Colab community.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"vineetvk/career1000\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "02226a60ca674e22baf8a2bb4121592a",
            "40f477daf85a45dd9ef4ec0d5853446d",
            "831ed1b4a991467786bce2478619ec44",
            "e85a463e277141f48ab8f218c5a3b6be",
            "672bea8b8f9f4643a48f2e734fd4bc0b",
            "5fdddcfbcc924493bfde90c7d0f2801e",
            "3c84a1cf817f445e89bdec3944c3ac6c",
            "654a81d6db8849008c9d1c11c217df21",
            "04396cea854048dcaf29791878ff184a",
            "81962322ff3c41b4956594dcff3c05f5",
            "7998a4e8e4b74917acd69590a2ab99ea",
            "0c8c6ecdedc1438589a7b0f786a2b65f",
            "373d78eae18b42f78f7e5caab0ce099f",
            "a4c0f7264efa481392df96f3a1d06488",
            "683655a77f5647f4a31941e50fb039e9",
            "1b6f0ad827f44b7896a906127025beaa",
            "8d4fc7c5f36f4970a497e37ac63dae8a",
            "f7ac5e5134a9457d8dac151439dc3239",
            "14adfb7127a1472ca4344e0d8356195f",
            "36c06bc3f5354ed8825e7f251626b32b",
            "24fb01783e854c74a74825c761de5bd7",
            "1e02cf5f9da24f51bbf9062a53899a67"
          ]
        },
        "id": "JgkkHOcx-MjD",
        "outputId": "4fab8ddf-4321-43e4-8719-3fdc76bfe02a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/33.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02226a60ca674e22baf8a2bb4121592a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c8c6ecdedc1438589a7b0f786a2b65f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5WcN8IqXAMvm"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}